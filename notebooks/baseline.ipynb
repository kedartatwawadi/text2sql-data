{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import dynet as dy # Loaded late to avoid memory allocation when we just want help info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Command line argument handling and default configuration ##\n",
    "\n",
    "abstract_lang = 'sql'\n",
    "###abstract_lang = 'logic'\n",
    "sys.argv = ['baseline_model','../data/atis.json','--do_test_eval']\n",
    "#--eval_freq 1000000 --log_freq 1000000 --max_bad_iters -1 --do_test_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='A simple template-based text-to-SQL system.')\n",
    "\n",
    "# IO\n",
    "parser.add_argument('data', help='Data in json format', nargs='+')\n",
    "parser.add_argument('--unk_max', help='Maximum count to be considered an unknown word', type=int, default=0)\n",
    "parser.add_argument('--query_split', help='Use the query split rather than the question split', action='store_true')\n",
    "parser.add_argument('--no_vars', help='Run without filling in variables', action='store_true')\n",
    "parser.add_argument('--use_all_sql', help='Default is to use first SQL only, this makes multiple instances.', action='store_true')\n",
    "parser.add_argument('--do_test_eval', help='Do the final evaluation on the test set (rather than dev).', action='store_true')\n",
    "parser.add_argument('--split', help='Use this split in cross-validation.', type=int)\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--mlp', help='Use a multi-layer perceptron', action='store_true')\n",
    "parser.add_argument('--dim_word', help='Dimensionality of word embeddings', type=int, default=128)\n",
    "parser.add_argument('--dim_hidden_lstm', help='Dimensionality of LSTM hidden vectors', type=int, default=64)\n",
    "parser.add_argument('--dim_hidden_mlp', help='Dimensionality of MLP hidden vectors', type=int, default=32)\n",
    "parser.add_argument('--dim_hidden_template', help='Dimensionality of MLP hidden vectors for the final template choice', type=int, default=64)\n",
    "parser.add_argument('--word_vectors', help='Pre-built word embeddings')\n",
    "parser.add_argument('--lstm_layers', help='Number of layers in the LSTM', type=int, default=2)\n",
    "\n",
    "# Training\n",
    "parser.add_argument('--max_iters', help='Maximum number of training iterations', type=int, default=22)\n",
    "parser.add_argument('--max_bad_iters', help='Maximum number of consecutive training iterations without improvement', type=int, default=10)\n",
    "parser.add_argument('--log_freq', help='Number of examples to decode between logging', type=int, default=1000000)\n",
    "parser.add_argument('--eval_freq', help='Number of examples to decode between evaluation runs', type=int, default=500000)\n",
    "parser.add_argument('--train_noise', help='Noise added to word embeddings as regularization', type=float, default=0.1)\n",
    "parser.add_argument('--lstm_dropout', help='Dropout for input and hidden elements of the LSTM', type=float, default=0.0)\n",
    "parser.add_argument('--learning_rate', help='Learning rate for optimiser', type=float, default=\"0.05\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input ##\n",
    "\n",
    "def insert_variables(sql, sql_variables, sent, sent_variables):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for token in sent.strip().split():\n",
    "        if (token not in sent_variables) or args.no_vars:\n",
    "            tokens.append(token)\n",
    "            tags.append(\"O\")\n",
    "        else:\n",
    "            assert len(sent_variables[token]) > 0\n",
    "            for word in sent_variables[token].split():\n",
    "                tokens.append(word)\n",
    "                tags.append(token)\n",
    "\n",
    "    sql_tokens = []\n",
    "    for token in sql.strip().split():\n",
    "        if token.startswith('\"%') or token.startswith(\"'%\"):\n",
    "            sql_tokens.append(token[:2])\n",
    "            token = token[2:]\n",
    "        elif token.startswith('\"') or token.startswith(\"'\"):\n",
    "            sql_tokens.append(token[0])\n",
    "            token = token[1:]\n",
    "\n",
    "        if token.endswith('%\"') or token.endswith(\"%'\"):\n",
    "            sql_tokens.append(token[:-2])\n",
    "            sql_tokens.append(token[-2:])\n",
    "        elif token.endswith('\"') or token.endswith(\"'\"):\n",
    "            sql_tokens.append(token[:-1])\n",
    "            sql_tokens.append(token[-1])\n",
    "        else:\n",
    "            sql_tokens.append(token)\n",
    "\n",
    "    template = []\n",
    "    for token in sql_tokens:\n",
    "        if (token not in sent_variables) and (token not in sql_variables):\n",
    "            template.append(token)\n",
    "        elif token in sent_variables:\n",
    "            if sent_variables[token] == '':\n",
    "                example = None\n",
    "                for variable in sql_variables:\n",
    "                    if variable['name'] == token:\n",
    "                        example = variable['example']\n",
    "                assert example is not None\n",
    "                template.append(example)\n",
    "            else:\n",
    "                template.append(token)\n",
    "        elif token in sql_variables:\n",
    "            example = None\n",
    "            for variable in sql_variables:\n",
    "                if variable['name'] == token:\n",
    "                    example = variable['example']\n",
    "            assert example is not None\n",
    "            template.append(example)\n",
    "            \n",
    "    template_tags = sorted(list(set(tags)))\n",
    "    return (tokens, tags, ' '.join(template) )\n",
    "\n",
    "def get_tagged_data_for_query(data):\n",
    "    dataset = data['query-split']\n",
    "    for sent_info in data['sentences']:\n",
    "        if not args.query_split:\n",
    "            dataset = sent_info['question-split']\n",
    "\n",
    "        if args.split is not None:\n",
    "            if str(args.split) == str(dataset):\n",
    "                dataset = \"test\"\n",
    "            else:\n",
    "                dataset = \"train\"\n",
    "\n",
    "        for sql in data[abstract_lang]:\n",
    "            sql_vars = data['variables']\n",
    "            text = sent_info['text']\n",
    "            text_vars = sent_info['variables']\n",
    "\n",
    "            yield (dataset, insert_variables(sql, sql_vars, text, text_vars))\n",
    "\n",
    "            if not args.use_all_sql:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "for filename in args.data:\n",
    "    with open(filename) as input_file:\n",
    "        data = json.load(input_file)\n",
    "        if type(data) == list:\n",
    "            for example in data:\n",
    "                for dataset, instance in get_tagged_data_for_query(example):\n",
    "                    if dataset == 'train':\n",
    "                        train.append(instance)\n",
    "                    elif dataset == 'dev':\n",
    "                        if args.do_test_eval:\n",
    "                            train.append(instance)\n",
    "                        else:\n",
    "                            dev.append(instance)\n",
    "                    elif dataset == 'test':\n",
    "                        test.append(instance)\n",
    "                    elif dataset == 'exclude':\n",
    "                        pass\n",
    "                    else:\n",
    "                        assert False, dataset\n",
    "        else:\n",
    "            for dataset, instance in get_tagged_data_for_query(data):\n",
    "                if dataset == 'train':\n",
    "                    train.append(instance)\n",
    "                elif dataset == 'dev':\n",
    "                    if args.do_test_eval:\n",
    "                        train.append(instance)\n",
    "                    else:\n",
    "                        dev.append(instance)\n",
    "                elif dataset == 'test':\n",
    "                    test.append(instance)\n",
    "                elif dataset == 'exclude':\n",
    "                    pass\n",
    "                else:\n",
    "                    assert False, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up voacbulary ##\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, w2i):\n",
    "        self.w2i = dict(w2i)\n",
    "        self.i2w = {i:w for w,i in w2i.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        w2i = {}\n",
    "        for word in corpus:\n",
    "            w2i.setdefault(word, len(w2i))\n",
    "        return Vocab(w2i)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.w2i.keys())\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    counts = Counter()\n",
    "    words = {\"<UNK>\"}\n",
    "    tag_set = set()\n",
    "    template_set = set()\n",
    "    template_to_tagset = {}\n",
    "    for tokens, tags, template in train:\n",
    "        template_set.add(template)\n",
    "        template_to_tagset[template] = set(tags)\n",
    "        for tag in tags:\n",
    "            tag_set.add(tag)\n",
    "        for token in tokens:\n",
    "            counts[token] += 1\n",
    "\n",
    "    for word in counts:\n",
    "        if counts[word] > args.unk_max:\n",
    "            words.add(word)\n",
    "\n",
    "    vocab_tags = Vocab.from_corpus(tag_set)\n",
    "    vocab_words = Vocab.from_corpus(words)\n",
    "    vocab_templates = Vocab.from_corpus(template_set)\n",
    "\n",
    "    return vocab_words, vocab_tags, vocab_templates, template_to_tagset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 871 templates\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags, vocab_templates,template_to_tagset = build_vocab(train)\n",
    "UNK = vocab_words.w2i[\"<UNK>\"]\n",
    "NWORDS = vocab_words.size()\n",
    "NTAGS = vocab_tags.size()\n",
    "NTEMPLATES = vocab_templates.size()\n",
    "\n",
    "print(\"Running with {} templates\".format(NTEMPLATES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up model ##\n",
    "\n",
    "model = dy.Model()\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=args.learning_rate)\n",
    "DIM_WORD = args.dim_word\n",
    "DIM_HIDDEN_LSTM = args.dim_hidden_lstm\n",
    "DIM_HIDDEN_MLP = args.dim_hidden_mlp\n",
    "DIM_HIDDEN_TEMPLATE = args.dim_hidden_template\n",
    "\n",
    "pEmbedding = model.add_lookup_parameters((NWORDS, DIM_WORD))\n",
    "if args.word_vectors is not None:\n",
    "    pretrained = []\n",
    "    with open(args.word_vectors,'rb') as pickleFile:\n",
    "        embedding = pickle.load(pickleFile)\n",
    "        for word_id in range(vocab_words.size()):\n",
    "            word = vocab_words.i2w[word_id]\n",
    "            if word in embedding:\n",
    "                pretrained.append(embedding[word])\n",
    "            else:\n",
    "                pretrained.append(pEmbedding.row_as_array(word_id))\n",
    "    pEmbedding.init_from_array(np.array(pretrained))\n",
    "if args.mlp:\n",
    "    pHidden = model.add_parameters((DIM_HIDDEN_MLP, DIM_HIDDEN_LSTM*2))\n",
    "    pOutput = model.add_parameters((NTAGS, DIM_HIDDEN_MLP))\n",
    "else:\n",
    "    pOutput = model.add_parameters((NTAGS, DIM_HIDDEN_LSTM*2))\n",
    "\n",
    "builders = [\n",
    "    dy.LSTMBuilder(args.lstm_layers, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "    dy.LSTMBuilder(args.lstm_layers, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "]\n",
    "\n",
    "pHiddenTemplate = model.add_parameters((DIM_HIDDEN_TEMPLATE, DIM_HIDDEN_LSTM*2))\n",
    "pOutputTemplate = model.add_parameters((NTEMPLATES, DIM_HIDDEN_TEMPLATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph_bkp(words, tags, template, builders, train=True,k=1):\n",
    "    dy.renew_cg()\n",
    "\n",
    "    if train and args.lstm_dropout is not None and args.lstm_dropout > 0:\n",
    "        for b in builders:\n",
    "            b.set_dropouts(args.lstm_dropout, args.lstm_dropout)\n",
    "\n",
    "    f_init, b_init = [b.initial_state() for b in builders]\n",
    "\n",
    "    wembs = [dy.lookup(pEmbedding, w) for w in words]\n",
    "    if train: # Add noise in training as a regularizer\n",
    "        wembs = [dy.noise(we, args.train_noise) for we in wembs]\n",
    "\n",
    "    fw_states = [x for x in f_init.add_inputs(wembs)]\n",
    "    bw_states = [x for x in b_init.add_inputs(reversed(wembs))]\n",
    "    fw = [x.output() for x in fw_states]\n",
    "    bw = [x.output() for x in bw_states]\n",
    "\n",
    "    O = dy.parameter(pOutput)\n",
    "    if args.mlp:\n",
    "        H = dy.parameter(pHidden)\n",
    "    errs = []\n",
    "    pred_tags = []\n",
    "    sorted_arg_topk = []\n",
    "    for f, b, t in zip(fw, reversed(bw), tags):\n",
    "        f_b = dy.concatenate([f,b])\n",
    "        if args.mlp:\n",
    "            f_b = dy.tanh(H * f_b)\n",
    "        r_t = O * f_b\n",
    "        if train:\n",
    "            err = dy.pickneglogsoftmax(r_t, t)\n",
    "            errs.append(err)\n",
    "        else:\n",
    "            out = dy.softmax(r_t)\n",
    "            chosen = np.argmax(out.npvalue())\n",
    "            pred_tags.append(vocab_tags.i2w[chosen])\n",
    "\n",
    "    O_template = dy.parameter(pOutputTemplate)\n",
    "    H_template = dy.parameter(pHiddenTemplate)\n",
    "    f_bt = dy.concatenate([fw_states[-1].s()[0], bw_states[-1].s()[0]])\n",
    "    f_bt = dy.tanh(H_template * f_bt)\n",
    "    r_tt = O_template * f_bt\n",
    "    pred_template = None\n",
    "    if train:\n",
    "        err = dy.pickneglogsoftmax(r_tt, template)\n",
    "        errs.append(err)\n",
    "    else:\n",
    "        out = dy.softmax(r_tt)\n",
    "        chosen = np.argmax(out.npvalue())        \n",
    "        pred_template = vocab_templates.i2w[chosen]\n",
    "        sorted_arg_topk = np.argsort(-out.npvalue())[:k]\n",
    "        \n",
    "    return pred_tags, pred_template, errs, sorted_arg_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(words, tags, template, builders, train=True,k=1):\n",
    "    dy.renew_cg()\n",
    "\n",
    "    if train and args.lstm_dropout is not None and args.lstm_dropout > 0:\n",
    "        for b in builders:\n",
    "            b.set_dropouts(args.lstm_dropout, args.lstm_dropout)\n",
    "\n",
    "    f_init, b_init = [b.initial_state() for b in builders]\n",
    "\n",
    "    wembs = [dy.lookup(pEmbedding, w) for w in words]\n",
    "    if train: # Add noise in training as a regularizer\n",
    "        wembs = [dy.noise(we, args.train_noise) for we in wembs]\n",
    "\n",
    "    fw_states = [x for x in f_init.add_inputs(wembs)]\n",
    "    bw_states = [x for x in b_init.add_inputs(reversed(wembs))]\n",
    "    fw = [x.output() for x in fw_states]\n",
    "    bw = [x.output() for x in bw_states]\n",
    "\n",
    "    O = dy.parameter(pOutput)\n",
    "    if args.mlp:\n",
    "        H = dy.parameter(pHidden)\n",
    "    errs = []\n",
    "    pred_tags = []\n",
    "    sorted_arg_topk = []\n",
    "    \n",
    "    sequences = [[list(), 0.0]]\n",
    "    for f, b, t in zip(fw, reversed(bw), tags):\n",
    "        f_b = dy.concatenate([f,b])\n",
    "        if args.mlp:\n",
    "            f_b = dy.tanh(H * f_b)\n",
    "        r_t = O * f_b\n",
    "        if train:\n",
    "            err = dy.pickneglogsoftmax(r_t, t)\n",
    "            errs.append(err)\n",
    "        else:\n",
    "            out = dy.softmax(r_t)\n",
    "            chosen = np.argmax(out.npvalue())\n",
    "            pred_tags.append(vocab_tags.i2w[chosen])\n",
    "\n",
    "    O_template = dy.parameter(pOutputTemplate)\n",
    "    H_template = dy.parameter(pHiddenTemplate)\n",
    "    f_bt = dy.concatenate([fw_states[-1].s()[0], bw_states[-1].s()[0]])\n",
    "    f_bt = dy.tanh(H_template * f_bt)\n",
    "    r_tt = O_template * f_bt\n",
    "    pred_template = None\n",
    "    if train:\n",
    "        err = dy.pickneglogsoftmax(r_tt, template)\n",
    "        errs.append(err)\n",
    "    else:\n",
    "        out = dy.softmax(r_tt)\n",
    "        chosen = np.argmax(out.npvalue())        \n",
    "        pred_template = vocab_templates.i2w[chosen]\n",
    "        sorted_arg_topk = np.argsort(-out.npvalue())[:k]\n",
    "        \n",
    "    return pred_tags, pred_template, errs, sorted_arg_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping at iter 10 as there have been 10 iters without improvement\n"
     ]
    }
   ],
   "source": [
    "tagged = 0\n",
    "loss = 0\n",
    "best_dev_acc = 0.0\n",
    "iters_since_best_updated = 0\n",
    "steps = 0\n",
    "for iteration in range(args.max_iters):\n",
    "    random.shuffle(train)\n",
    "    for tokens, tags, template in train:\n",
    "        steps += 1\n",
    "\n",
    "        # Convert to indices\n",
    "        word_ids = [vocab_words.w2i.get(word, UNK) for word in tokens]\n",
    "        tag_ids = [vocab_tags.w2i[tag] for tag in tags]\n",
    "        template_id = vocab_templates.w2i[template]\n",
    "\n",
    "        # Decode and update\n",
    "        _, _, errs,_ = build_tagging_graph(word_ids, tag_ids, template_id, builders)\n",
    "        sum_errs = dy.esum(errs)\n",
    "        loss += sum_errs.scalar_value()\n",
    "        tagged += len(tag_ids)\n",
    "        sum_errs.backward()\n",
    "        trainer.update()\n",
    "\n",
    "        # Log status\n",
    "        if steps % args.log_freq == 0:\n",
    "            trainer.status()\n",
    "            print(\"TrainLoss {}-{}: {}\".format(iteration, steps, loss / tagged))\n",
    "            loss = 0\n",
    "            tagged = 0\n",
    "            sys.stdout.flush()\n",
    "        if steps % args.eval_freq == 0:\n",
    "            acc = run_eval(dev, builders, iteration, steps)\n",
    "            if best_dev_acc < acc:\n",
    "                best_dev_acc = acc\n",
    "                iters_since_best_updated = 0\n",
    "                print(\"New best Acc!\", acc)\n",
    "            sys.stdout.flush()\n",
    "    iters_since_best_updated += 1\n",
    "    if args.max_bad_iters > 0 and iters_since_best_updated > args.max_bad_iters:\n",
    "        print(\"Stopping at iter {} as there have been {} iters without improvement\".format(iteration, args.max_bad_iters))\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(data, builders, iteration, step,k):\n",
    "    if len(data) == 0:\n",
    "        print(\"No data for eval\")\n",
    "        return -1\n",
    "    good = 0.0\n",
    "    total = 0.0\n",
    "    complete_good = 0.0\n",
    "    templates_good = 0.0\n",
    "    oracle = 0.0\n",
    "    tag_set_wrong = 0.0\n",
    "    for tokens, tags, template in data:\n",
    "        word_ids = [vocab_words.w2i.get(word, UNK) for word in tokens]\n",
    "        tag_ids = [0 for tag in tags]\n",
    "        pred_tags, pred_template, _,pred_arg_topk = build_tagging_graph(word_ids, tag_ids, 0, builders, False,k=k)\n",
    "        gold_tags = tags\n",
    "        perfect = True\n",
    "        \n",
    "        \n",
    "        for gold, pred in zip(gold_tags, pred_tags):\n",
    "            total += 1\n",
    "            if gold == pred: \n",
    "                good += 1\n",
    "            else:\n",
    "                perfect = False\n",
    "                \n",
    "        \n",
    "        \n",
    "        for template_id in pred_arg_topk:\n",
    "            pred_template = vocab_templates.i2w[template_id]\n",
    "            if template_to_tagset[pred_template] == set(pred_tags):\n",
    "                break;\n",
    "            \n",
    "        if template_to_tagset[pred_template] != set(pred_tags):\n",
    "            tag_set_wrong += 1\n",
    "        \n",
    "        if pred_template == template:\n",
    "            templates_good += 1\n",
    "            if perfect:\n",
    "                complete_good += 1\n",
    "        if template in vocab_templates.w2i:\n",
    "            oracle += 1\n",
    "    tok_acc = good / total\n",
    "    \n",
    "    tagset_acc = tag_set_wrong/ len(data)\n",
    "    complete_acc = complete_good / len(data)\n",
    "    template_acc = templates_good / len(data)\n",
    "    oracle_acc = oracle / len(data)\n",
    "    print(\"Eval {}-{} Acc: {:>5} Tmpl: {:<5} Tot: {:>5} Orcl: {:>5} Tag: {:>5}\".format(iteration, step, tok_acc, template_acc, complete_acc, oracle_acc, tagset_acc))\n",
    "    return complete_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval End-test Acc:   1.0 Tmpl: 1.0   Tot:   1.0 Orcl:   1.0 Tag:   0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_eval(test[:60], builders, \"End\", \"test\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4966442953020134 Tot: 0.4541387024608501 Orcl: 0.6935123042505593 Tag: 0.3042505592841163\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4966442953020134 Tot: 0.4541387024608501 Orcl: 0.6935123042505593 Tag: 0.3042505592841163\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4697986577181208 Tot: 0.44966442953020136 Orcl: 0.6935123042505593 Tag: 0.24161073825503357\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.47874720357941836 Tot: 0.45861297539149887 Orcl: 0.6935123042505593 Tag: 0.1610738255033557\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.48769574944071586 Tot: 0.46756152125279643 Orcl: 0.6935123042505593 Tag: 0.1342281879194631\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.48769574944071586 Tot: 0.46756152125279643 Orcl: 0.6935123042505593 Tag: 0.12304250559284116\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4899328859060403 Tot: 0.4697986577181208 Orcl: 0.6935123042505593 Tag: 0.10961968680089486\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4899328859060403 Tot: 0.4697986577181208 Orcl: 0.6935123042505593 Tag: 0.10738255033557047\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4899328859060403 Tot: 0.4697986577181208 Orcl: 0.6935123042505593 Tag: 0.09843400447427293\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4899328859060403 Tot: 0.4697986577181208 Orcl: 0.6935123042505593 Tag: 0.09619686800894854\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4899328859060403 Tot: 0.4697986577181208 Orcl: 0.6935123042505593 Tag: 0.09395973154362416\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4899328859060403 Tot: 0.4697986577181208 Orcl: 0.6935123042505593 Tag: 0.09395973154362416\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4899328859060403 Tot: 0.4697986577181208 Orcl: 0.6935123042505593 Tag: 0.09172259507829977\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.4899328859060403 Tot: 0.4697986577181208 Orcl: 0.6935123042505593 Tag: 0.087248322147651\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.49217002237136465 Tot: 0.4720357941834452 Orcl: 0.6935123042505593 Tag: 0.08501118568232663\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.49217002237136465 Tot: 0.4720357941834452 Orcl: 0.6935123042505593 Tag: 0.08501118568232663\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.49217002237136465 Tot: 0.4720357941834452 Orcl: 0.6935123042505593 Tag: 0.08501118568232663\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.49217002237136465 Tot: 0.4720357941834452 Orcl: 0.6935123042505593 Tag: 0.08277404921700224\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.49217002237136465 Tot: 0.4720357941834452 Orcl: 0.6935123042505593 Tag: 0.08277404921700224\n",
      "Eval End-test Acc: 0.9613207547169811 Tmpl: 0.49217002237136465 Tot: 0.4720357941834452 Orcl: 0.6935123042505593 Tag: 0.07829977628635347\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    run_eval(test, builders, \"End\", \"test\",k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
