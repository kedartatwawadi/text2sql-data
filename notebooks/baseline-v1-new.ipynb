{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import dynet as dy # Loaded late to avoid memory allocation when we just want help info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Command line argument handling and default configuration ##\n",
    "\n",
    "abstract_lang = 'sql'\n",
    "###abstract_lang = 'logic'\n",
    "sys.argv = ['baseline_model','../data/atis.json','--do_test_eval']\n",
    "#--eval_freq 1000000 --log_freq 1000000 --max_bad_iters -1 --do_test_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='A simple template-based text-to-SQL system.')\n",
    "\n",
    "# IO\n",
    "parser.add_argument('data', help='Data in json format', nargs='+')\n",
    "parser.add_argument('--unk_max', help='Maximum count to be considered an unknown word', type=int, default=0)\n",
    "parser.add_argument('--query_split', help='Use the query split rather than the question split', action='store_true')\n",
    "parser.add_argument('--no_vars', help='Run without filling in variables', action='store_true')\n",
    "parser.add_argument('--use_all_sql', help='Default is to use first SQL only, this makes multiple instances.', action='store_true')\n",
    "parser.add_argument('--do_test_eval', help='Do the final evaluation on the test set (rather than dev).', action='store_true')\n",
    "parser.add_argument('--split', help='Use this split in cross-validation.', type=int)\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--mlp', help='Use a multi-layer perceptron', action='store_true')\n",
    "parser.add_argument('--dim_word', help='Dimensionality of word embeddings', type=int, default=128)\n",
    "parser.add_argument('--dim_hidden_lstm', help='Dimensionality of LSTM hidden vectors', type=int, default=64)\n",
    "parser.add_argument('--dim_hidden_mlp', help='Dimensionality of MLP hidden vectors', type=int, default=32)\n",
    "parser.add_argument('--dim_hidden_template', help='Dimensionality of MLP hidden vectors for the final template choice', type=int, default=64)\n",
    "parser.add_argument('--word_vectors', help='Pre-built word embeddings')\n",
    "parser.add_argument('--lstm_layers', help='Number of layers in the LSTM', type=int, default=2)\n",
    "\n",
    "# Training\n",
    "parser.add_argument('--max_iters', help='Maximum number of training iterations', type=int, default=22)\n",
    "parser.add_argument('--max_bad_iters', help='Maximum number of consecutive training iterations without improvement', type=int, default=2)\n",
    "parser.add_argument('--log_freq', help='Number of examples to decode between logging', type=int, default=1000000)\n",
    "parser.add_argument('--eval_freq', help='Number of examples to decode between evaluation runs', type=int, default=500000)\n",
    "parser.add_argument('--train_noise', help='Noise added to word embeddings as regularization', type=float, default=0.1)\n",
    "parser.add_argument('--lstm_dropout', help='Dropout for input and hidden elements of the LSTM', type=float, default=0.0)\n",
    "parser.add_argument('--learning_rate', help='Learning rate for optimiser', type=float, default=\"0.05\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input ##\n",
    "\n",
    "def insert_variables(sql, sql_variables, sent, sent_variables):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for token in sent.strip().split():\n",
    "        if (token not in sent_variables) or args.no_vars:\n",
    "            tokens.append(token)\n",
    "            tags.append(\"O\")\n",
    "        else:\n",
    "            assert len(sent_variables[token]) > 0\n",
    "            for word in sent_variables[token].split():\n",
    "                tokens.append(word)\n",
    "                tags.append(token)\n",
    "\n",
    "    sql_tokens = []\n",
    "    for token in sql.strip().split():\n",
    "        if token.startswith('\"%') or token.startswith(\"'%\"):\n",
    "            sql_tokens.append(token[:2])\n",
    "            token = token[2:]\n",
    "        elif token.startswith('\"') or token.startswith(\"'\"):\n",
    "            sql_tokens.append(token[0])\n",
    "            token = token[1:]\n",
    "\n",
    "        if token.endswith('%\"') or token.endswith(\"%'\"):\n",
    "            sql_tokens.append(token[:-2])\n",
    "            sql_tokens.append(token[-2:])\n",
    "        elif token.endswith('\"') or token.endswith(\"'\"):\n",
    "            sql_tokens.append(token[:-1])\n",
    "            sql_tokens.append(token[-1])\n",
    "        else:\n",
    "            sql_tokens.append(token)\n",
    "\n",
    "    template = []\n",
    "    for token in sql_tokens:\n",
    "        if (token not in sent_variables) and (token not in sql_variables):\n",
    "            template.append(token)\n",
    "        elif token in sent_variables:\n",
    "            if sent_variables[token] == '':\n",
    "                example = None\n",
    "                for variable in sql_variables:\n",
    "                    if variable['name'] == token:\n",
    "                        example = variable['example']\n",
    "                assert example is not None\n",
    "                template.append(example)\n",
    "            else:\n",
    "                template.append(token)\n",
    "        elif token in sql_variables:\n",
    "            example = None\n",
    "            for variable in sql_variables:\n",
    "                if variable['name'] == token:\n",
    "                    example = variable['example']\n",
    "            assert example is not None\n",
    "            template.append(example)\n",
    "            \n",
    "    template_tags = sorted(list(set(tags)))\n",
    "    return (tokens, tags, ' '.join(template), sent_variables )\n",
    "\n",
    "def get_tagged_data_for_query(data):\n",
    "    dataset = data['query-split']\n",
    "    for sent_info in data['sentences']:\n",
    "        if not args.query_split:\n",
    "            dataset = sent_info['question-split']\n",
    "\n",
    "        if args.split is not None:\n",
    "            if str(args.split) == str(dataset):\n",
    "                dataset = \"test\"\n",
    "            else:\n",
    "                dataset = \"train\"\n",
    "\n",
    "        for sql in data[abstract_lang]:\n",
    "            sql_vars = data['variables']\n",
    "            text = sent_info['text']\n",
    "            text_vars = sent_info['variables']\n",
    "\n",
    "            yield (dataset, insert_variables(sql, sql_vars, text, text_vars))\n",
    "\n",
    "            if not args.use_all_sql:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "for filename in args.data:\n",
    "    with open(filename) as input_file:\n",
    "        data = json.load(input_file)\n",
    "        if type(data) == list:\n",
    "            for example in data:\n",
    "                for dataset, instance in get_tagged_data_for_query(example):\n",
    "                    if dataset == 'train':\n",
    "                        train.append(instance)\n",
    "                    elif dataset == 'dev':\n",
    "                        if args.do_test_eval:\n",
    "                            train.append(instance)\n",
    "                        else:\n",
    "                            dev.append(instance)\n",
    "                    elif dataset == 'test':\n",
    "                        test.append(instance)\n",
    "                    elif dataset == 'exclude':\n",
    "                        pass\n",
    "                    else:\n",
    "                        assert False, dataset\n",
    "        else:\n",
    "            for dataset, instance in get_tagged_data_for_query(data):\n",
    "                if dataset == 'train':\n",
    "                    train.append(instance)\n",
    "                elif dataset == 'dev':\n",
    "                    if args.do_test_eval:\n",
    "                        train.append(instance)\n",
    "                    else:\n",
    "                        dev.append(instance)\n",
    "                elif dataset == 'test':\n",
    "                    test.append(instance)\n",
    "                elif dataset == 'exclude':\n",
    "                    pass\n",
    "                else:\n",
    "                    assert False, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up voacbulary ##\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, w2i):\n",
    "        self.w2i = dict(w2i)\n",
    "        self.i2w = {i:w for w,i in w2i.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        w2i = {}\n",
    "        for word in corpus:\n",
    "            w2i.setdefault(word, len(w2i))\n",
    "        return Vocab(w2i)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.w2i.keys())\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    counts = Counter()\n",
    "    words = {\"<UNK>\"}\n",
    "    tag_set = set()\n",
    "    template_set = set()\n",
    "    template_to_tagset = {}\n",
    "    for tokens, tags, template, text_variables in train:\n",
    "        template_set.add(template)\n",
    "        template_to_tagset[template] = set(text_variables.keys())\n",
    "        for tag in tags:\n",
    "            tag_set.add(tag)\n",
    "        for token in tokens:\n",
    "            counts[token] += 1\n",
    "\n",
    "    for word in counts:\n",
    "        if counts[word] > args.unk_max:\n",
    "            words.add(word)\n",
    "\n",
    "    vocab_tags = Vocab.from_corpus(tag_set)\n",
    "    vocab_words = Vocab.from_corpus(words)\n",
    "    vocab_templates = Vocab.from_corpus(template_set)\n",
    "\n",
    "    return vocab_words, vocab_tags, vocab_templates, template_to_tagset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 871 templates\n"
     ]
    }
   ],
   "source": [
    "vocab_words, vocab_tags, vocab_templates,template_to_tagset = build_vocab(train)\n",
    "UNK = vocab_words.w2i[\"<UNK>\"]\n",
    "NWORDS = vocab_words.size()\n",
    "NTAGS = vocab_tags.size()\n",
    "NTEMPLATES = vocab_templates.size()\n",
    "\n",
    "print(\"Running with {} templates\".format(NTEMPLATES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "template_set_test = set()\n",
    "count = 0\n",
    "for tokens, tags, template, text_variables in test:\n",
    "    if template not in template_to_tagset:\n",
    "        template_set_test.add(template)\n",
    "        template_to_tagset[template] = set(text_variables.keys())\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up model ##\n",
    "\n",
    "model = dy.Model()\n",
    "trainer = dy.SimpleSGDTrainer(model, learning_rate=args.learning_rate)\n",
    "DIM_WORD = args.dim_word\n",
    "DIM_HIDDEN_LSTM = args.dim_hidden_lstm\n",
    "DIM_HIDDEN_MLP = args.dim_hidden_mlp\n",
    "DIM_HIDDEN_TEMPLATE = args.dim_hidden_template\n",
    "\n",
    "pEmbedding = model.add_lookup_parameters((NWORDS, DIM_WORD))\n",
    "if args.word_vectors is not None:\n",
    "    pretrained = []\n",
    "    with open(args.word_vectors,'rb') as pickleFile:\n",
    "        embedding = pickle.load(pickleFile)\n",
    "        for word_id in range(vocab_words.size()):\n",
    "            word = vocab_words.i2w[word_id]\n",
    "            if word in embedding:\n",
    "                pretrained.append(embedding[word])\n",
    "            else:\n",
    "                pretrained.append(pEmbedding.row_as_array(word_id))\n",
    "    pEmbedding.init_from_array(np.array(pretrained))\n",
    "if args.mlp:\n",
    "    pHidden = model.add_parameters((DIM_HIDDEN_MLP, DIM_HIDDEN_LSTM*2))\n",
    "    pOutput = model.add_parameters((NTAGS, DIM_HIDDEN_MLP))\n",
    "else:\n",
    "    pOutput = model.add_parameters((NTAGS, DIM_HIDDEN_LSTM*2))\n",
    "\n",
    "builders = [\n",
    "    dy.LSTMBuilder(args.lstm_layers, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "    dy.LSTMBuilder(args.lstm_layers, DIM_WORD, DIM_HIDDEN_LSTM, model),\n",
    "]\n",
    "\n",
    "pHiddenTemplate = model.add_parameters((DIM_HIDDEN_TEMPLATE, DIM_HIDDEN_LSTM*2))\n",
    "pOutputTemplate = model.add_parameters((NTEMPLATES, DIM_HIDDEN_TEMPLATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagging_graph(words, tags, template, builders, train=True,k=1):\n",
    "    dy.renew_cg()\n",
    "\n",
    "    if train and args.lstm_dropout is not None and args.lstm_dropout > 0:\n",
    "        for b in builders:\n",
    "            b.set_dropouts(args.lstm_dropout, args.lstm_dropout)\n",
    "\n",
    "    f_init, b_init = [b.initial_state() for b in builders]\n",
    "\n",
    "    wembs = [dy.lookup(pEmbedding, w) for w in words]\n",
    "    if train: # Add noise in training as a regularizer\n",
    "        wembs = [dy.noise(we, args.train_noise) for we in wembs]\n",
    "\n",
    "    fw_states = [x for x in f_init.add_inputs(wembs)]\n",
    "    bw_states = [x for x in b_init.add_inputs(reversed(wembs))]\n",
    "    fw = [x.output() for x in fw_states]\n",
    "    bw = [x.output() for x in bw_states]\n",
    "\n",
    "    O = dy.parameter(pOutput)\n",
    "    if args.mlp:\n",
    "        H = dy.parameter(pHidden)\n",
    "    errs = []\n",
    "    pred_tags = []\n",
    "    sorted_arg_topk = []\n",
    "    final_topk = []\n",
    "    sequences_topk = [(0.0,list())]\n",
    "    for f, b, t in zip(fw, reversed(bw), tags):\n",
    "        f_b = dy.concatenate([f,b])\n",
    "        if args.mlp:\n",
    "            f_b = dy.tanh(H * f_b)\n",
    "        r_t = O * f_b\n",
    "        if train:\n",
    "            err = dy.pickneglogsoftmax(r_t, t)\n",
    "            errs.append(err)\n",
    "        else:\n",
    "            out = dy.log_softmax(r_t)\n",
    "            chosen = np.argmax(out.npvalue())\n",
    "            pred_tags.append(vocab_tags.i2w[chosen])\n",
    "            all_sequences = list()\n",
    "            for seq in sequences_topk:\n",
    "                seq_score,seq_list = seq\n",
    "                _scores = -out.npvalue()\n",
    "                arg_topk = np.argsort(_scores)[:k]\n",
    "                score_topk = _scores[arg_topk]\n",
    "                for i in range(k):\n",
    "                    _list = list(seq_list)\n",
    "                    _list.append(vocab_tags.i2w[arg_topk[i]])\n",
    "                    score = seq_score + score_topk[i]\n",
    "                    all_sequences.append((score,_list))\n",
    "            sequences_topk = sorted(all_sequences)[:k]\n",
    "            \n",
    "\n",
    "    O_template = dy.parameter(pOutputTemplate)\n",
    "    H_template = dy.parameter(pHiddenTemplate)\n",
    "    f_bt = dy.concatenate([fw_states[-1].s()[0], bw_states[-1].s()[0]])\n",
    "    f_bt = dy.tanh(H_template * f_bt)\n",
    "    r_tt = O_template * f_bt\n",
    "    pred_template = None\n",
    "    if train:\n",
    "        err = dy.pickneglogsoftmax(r_tt, template)\n",
    "        errs.append(err)\n",
    "    else:\n",
    "        out = dy.log_softmax(r_tt)\n",
    "        _scores = -out.npvalue()\n",
    "        chosen = np.argmin(_scores)        \n",
    "        pred_template = vocab_templates.i2w[chosen]\n",
    "        sorted_arg_topk = np.argsort(_scores)[:k]\n",
    "        \n",
    "        all_sequences_and_templates = []\n",
    "        for template_id in sorted_arg_topk:\n",
    "            _score = _scores[template_id]\n",
    "            _template = vocab_templates.i2w[template_id]\n",
    "\n",
    "            for seq_score,seq_list in sequences_topk:\n",
    "                all_sequences_and_templates.append((_score + seq_score, seq_list, _template))\n",
    "        final_topk = sorted(all_sequences_and_templates)[:k]    \n",
    "        \n",
    "        \n",
    "    return pred_tags, pred_template, errs, final_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dy.parameter(...) call is now DEPRECATED.\n",
      "        There is no longer need to explicitly add parameters to the computation graph.\n",
      "        Any used parameter will be added automatically.\n",
      "Stopping at iter 2 as there have been 2 iters without improvement\n"
     ]
    }
   ],
   "source": [
    "tagged = 0\n",
    "loss = 0\n",
    "best_dev_acc = 0.0\n",
    "iters_since_best_updated = 0\n",
    "steps = 0\n",
    "for iteration in range(args.max_iters):\n",
    "    random.shuffle(train)\n",
    "    for tokens, tags, template, text_variables in train:\n",
    "        steps += 1\n",
    "\n",
    "        # Convert to indices\n",
    "        word_ids = [vocab_words.w2i.get(word, UNK) for word in tokens]\n",
    "        tag_ids = [vocab_tags.w2i[tag] for tag in tags]\n",
    "        template_id = vocab_templates.w2i[template]\n",
    "\n",
    "        # Decode and update\n",
    "        _, _, errs,_ = build_tagging_graph(word_ids, tag_ids, template_id, builders)\n",
    "        sum_errs = dy.esum(errs)\n",
    "        loss += sum_errs.scalar_value()\n",
    "        tagged += len(tag_ids)\n",
    "        sum_errs.backward()\n",
    "        trainer.update()\n",
    "\n",
    "        # Log status\n",
    "        if steps % args.log_freq == 0:\n",
    "            trainer.status()\n",
    "            print(\"TrainLoss {}-{}: {}\".format(iteration, steps, loss / tagged))\n",
    "            loss = 0\n",
    "            tagged = 0\n",
    "            sys.stdout.flush()\n",
    "        if steps % args.eval_freq == 0:\n",
    "            acc = run_eval(dev, builders, iteration, steps)\n",
    "            if best_dev_acc < acc:\n",
    "                best_dev_acc = acc\n",
    "                iters_since_best_updated = 0\n",
    "                print(\"New best Acc!\", acc)\n",
    "            sys.stdout.flush()\n",
    "    iters_since_best_updated += 1\n",
    "    if args.max_bad_iters > 0 and iters_since_best_updated > args.max_bad_iters:\n",
    "        print(\"Stopping at iter {} as there have been {} iters without improvement\".format(iteration, args.max_bad_iters))\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('../data/atis-sqlite.db')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting records\n",
      "  Downloading https://files.pythonhosted.org/packages/74/4d/4f16ebe391476e0212b3285454afa82f98a7c2466ecd88f90736c60e22ac/records-0.5.2-py2.py3-none-any.whl\n",
      "Collecting SQLAlchemy (from records)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/c2/29491103fd971f3988e90ee3a77bb58bad2ae2acd6e8ea30a6d1432c33a3/SQLAlchemy-1.2.10.tar.gz (5.6MB)\n",
      "\u001b[K    100% |################################| 5.6MB 279kB/s eta 0:00:01    32% |##########                      | 1.8MB 20.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tablib (from records)\n",
      "  Downloading https://files.pythonhosted.org/packages/e4/9f/cba4e1145ca9ec84d9326f7ce38c6b5f37d9be8bc1af1bd8b19c20374095/tablib-0.12.1.tar.gz (63kB)\n",
      "\u001b[K    100% |################################| 71kB 11.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting docopt (from records)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Collecting odfpy (from tablib->records)\n",
      "  Downloading https://files.pythonhosted.org/packages/01/0f/c9971c99d0d06024a1652f467427ff3f1a1136237e5740da715c5b208a48/odfpy-1.3.6.tar.gz (691kB)\n",
      "\u001b[K    100% |################################| 696kB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting openpyxl (from tablib->records)\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/dd/5952829956827de7ff36eb70877fdffd6dbfacb670fae05eb7ccba52ace7/openpyxl-2.5.5.tar.gz (171kB)\n",
      "\u001b[K    100% |################################| 174kB 7.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting unicodecsv (from tablib->records)\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
      "Collecting xlrd (from tablib->records)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/e6/e95c4eec6221bfd8528bcc4ea252a850bffcc4be88ebc367e23a1a84b0bb/xlrd-1.1.0-py2.py3-none-any.whl (108kB)\n",
      "\u001b[K    100% |################################| 112kB 11.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting xlwt (from tablib->records)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/48/def306413b25c3d01753603b1a222a011b8621aed27cd7f89cbc27e6b0f4/xlwt-1.3.0-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% |################################| 102kB 9.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): pyyaml in /opt/conda/lib/python3.5/site-packages (from tablib->records)\n",
      "Collecting jdcal (from openpyxl->tablib->records)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/38/dcf83532480f25284f3ef13f8ed63e03c58a65c9d3ba2a6a894ed9497207/jdcal-1.4-py2.py3-none-any.whl\n",
      "Collecting et_xmlfile (from openpyxl->tablib->records)\n",
      "  Downloading https://files.pythonhosted.org/packages/22/28/a99c42aea746e18382ad9fb36f64c1c1f04216f41797f2f0fa567da11388/et_xmlfile-1.0.1.tar.gz\n",
      "Building wheels for collected packages: SQLAlchemy, tablib, docopt, odfpy, openpyxl, unicodecsv, et-xmlfile\n",
      "  Running setup.py bdist_wheel for SQLAlchemy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3b/5e/6b/6716bdf0815da6a7e4526c1f39d6e7d8cbb5eb5054f83f98fb\n",
      "  Running setup.py bdist_wheel for tablib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/e4/b9/9eb46eaf91476d5dd88a5cf6f76f0884114eafe0b2ef24e2df\n",
      "  Running setup.py bdist_wheel for docopt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "  Running setup.py bdist_wheel for odfpy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/da/af/f6/a0b436814a73a88125b8fa82acc73ae96de511bc1faa62b7cc\n",
      "  Running setup.py bdist_wheel for openpyxl ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b8/20/5b/d260e131180f4394eba48d97e238016f4bde050727fce79283\n",
      "  Running setup.py bdist_wheel for unicodecsv ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
      "  Running setup.py bdist_wheel for et-xmlfile ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2a/77/35/0da0965a057698121fc7d8c5a7a9955cdbfb3cc4e2423cad39\n",
      "Successfully built SQLAlchemy tablib docopt odfpy openpyxl unicodecsv et-xmlfile\n",
      "Installing collected packages: SQLAlchemy, odfpy, jdcal, et-xmlfile, openpyxl, unicodecsv, xlrd, xlwt, tablib, docopt, records\n",
      "Successfully installed SQLAlchemy-1.2.10 docopt-0.6.2 et-xmlfile-1.0.1 jdcal-1.4 odfpy-1.3.6 openpyxl-2.5.5 records-0.5.2 tablib-0.12.1 unicodecsv-0.14.1 xlrd-1.1.0 xlwt-1.3.0\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_from_token_template(_tokens,_tags,template):\n",
    "    sql = str(template)\n",
    "    tokens = list(_tokens) \n",
    "    tags = list(_tags)\n",
    "    tokens.append(\"test\")\n",
    "    tags.append(\"O\")\n",
    "    _curr_tag = \"\"\n",
    "    _curr_token = \"\"\n",
    "    for _token,_tag in zip(tokens,tags):\n",
    "        if _tag is _curr_tag:\n",
    "            _curr_token = _curr_token + \" \" + _token\n",
    "        else:\n",
    "            if _curr_tag is 'O':\n",
    "                pass\n",
    "            else:\n",
    "                if _curr_token.isdigit():\n",
    "                    sql = sql.replace(_curr_tag,_curr_token)\n",
    "                else:\n",
    "                    sql = sql.replace(\" \" + _curr_tag + \" \",_curr_token)\n",
    "            _curr_tag = _tag\n",
    "            _curr_token = _token\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(data, builders, iteration, step,k):\n",
    "    if len(data) == 0:\n",
    "        print(\"No data for eval\")\n",
    "        return -1\n",
    "    good = 0.0\n",
    "    total = 0.0\n",
    "    complete_good = 0.0\n",
    "    templates_good = 0.0\n",
    "    oracle = 0.0\n",
    "    tag_set_wrong = 0.0\n",
    "    inconsistent_examples = 0\n",
    "    template_error = 0.0\n",
    "    tags_good = 0.0\n",
    "    for tokens, tags, template, text_variables in data:\n",
    "        word_ids = [vocab_words.w2i.get(word, UNK) for word in tokens]\n",
    "        tag_ids = [0 for tag in tags]\n",
    "        pred_tags, pred_template, _ , final_topk = build_tagging_graph(word_ids, tag_ids, 0, builders, False,k=k)\n",
    "        gold_tags = tags\n",
    "        tags_correct = True\n",
    "        template_correct = False\n",
    "        inconsistent = True\n",
    "        only_test_template = False\n",
    "        \n",
    "        if template in template_set_test:\n",
    "            only_test_template = True\n",
    "            template_error += 1.0\n",
    "        else:\n",
    "            _temp = template_to_tagset[template]\n",
    "            if _temp != set(gold_tags):\n",
    "                inconsistent_examples += 1.0\n",
    "            else:\n",
    "                inconsistent = False\n",
    "        \n",
    "        for score, _tags, _template in final_topk:\n",
    "            if template_to_tagset[_template] == set(_tags):\n",
    "                exception = 0\n",
    "                try:\n",
    "                    sql_query = get_sql_from_token_template(tokens,_tags,_template)\n",
    "                    c.execute(sql_query)\n",
    "                except Exception as e:\n",
    "                    exception = 1\n",
    "                \n",
    "                if not exception:\n",
    "                    pred_tags = _tags\n",
    "                    pred_template = _template\n",
    "                    break;\n",
    "        \n",
    "        for gold, pred in zip(gold_tags, pred_tags):\n",
    "            total += 1\n",
    "            if gold == pred: \n",
    "                good += 1\n",
    "            else:\n",
    "                tags_correct = False\n",
    "                \n",
    "        if gold_tags == pred_tags:\n",
    "            tags_good += 1\n",
    "            \n",
    "        if template_to_tagset[pred_template] != set(pred_tags):\n",
    "            tag_set_wrong += 1\n",
    "        \n",
    "        if pred_template == template:\n",
    "            templates_good += 1\n",
    "            template_correct = True\n",
    "            \n",
    "        if tags_correct and template_correct and (not inconsistent):\n",
    "            complete_good += 1\n",
    "        \n",
    "        if template in vocab_templates.w2i:\n",
    "            oracle += 1\n",
    "    tok_acc = good / total\n",
    "    \n",
    "    tagset_acc = tag_set_wrong/ len(data)\n",
    "    complete_acc = complete_good / len(data)\n",
    "    template_acc = templates_good / (len(data) - template_error)\n",
    "    tag_acc = tags_good/len(data)\n",
    "    print(\"Inconsistent for copy mechanism: \", inconsistent_examples*1.0/len(data))\n",
    "    print(\"Template only in Test data: \", template_error*1.0/len(data))\n",
    "    print(\"Tok-Acc: {:>5} Tags: {:<5} Tmpl: {:<5} Tot: {:>5} Tag: {:>5}\".format( tok_acc, tag_acc, template_acc, complete_acc, tagset_acc))\n",
    "    return complete_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistent for copy mechanism:  0.5\n",
      "Template only in Test data:  0.0\n",
      "Tok-Acc: 0.9867172675521821 Tags: 0.9166666666666666 Tmpl: 0.9   Tot:  0.35 Tag:  0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_eval(test[:60], builders, \"End\", \"test\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistent for copy mechanism:  0.19015659955257272\n",
      "Template only in Test data:  0.30648769574944074\n",
      "Tok-Acc: 0.9375 Tags: 0.6532438478747203 Tmpl: 0.5870967741935483 Tot: 0.25727069351230425 Tag: 0.6353467561521253\n",
      "Inconsistent for copy mechanism:  0.19015659955257272\n",
      "Template only in Test data:  0.30648769574944074\n",
      "Tok-Acc: 0.9408018867924528 Tags: 0.6935123042505593 Tmpl: 0.5838709677419355 Tot: 0.2975391498881432 Tag: 0.4742729306487696\n",
      "Inconsistent for copy mechanism:  0.19015659955257272\n",
      "Template only in Test data:  0.30648769574944074\n",
      "Tok-Acc: 0.9412735849056604 Tags: 0.6935123042505593 Tmpl: 0.5774193548387097 Tot: 0.30201342281879195 Tag: 0.38478747203579416\n",
      "Inconsistent for copy mechanism:  0.19015659955257272\n",
      "Template only in Test data:  0.30648769574944074\n",
      "Tok-Acc: 0.9410377358490566 Tags: 0.6912751677852349 Tmpl: 0.5774193548387097 Tot: 0.3087248322147651 Tag: 0.3400447427293065\n",
      "Inconsistent for copy mechanism:  0.19015659955257272\n",
      "Template only in Test data:  0.30648769574944074\n",
      "Tok-Acc: 0.9410377358490566 Tags: 0.6912751677852349 Tmpl: 0.5741935483870968 Tot: 0.3087248322147651 Tag: 0.3221476510067114\n",
      "Inconsistent for copy mechanism:  0.19015659955257272\n",
      "Template only in Test data:  0.30648769574944074\n",
      "Tok-Acc: 0.9410377358490566 Tags: 0.6912751677852349 Tmpl: 0.5806451612903226 Tot: 0.3131991051454139 Tag: 0.3131991051454139\n",
      "Inconsistent for copy mechanism:  0.19015659955257272\n",
      "Template only in Test data:  0.30648769574944074\n",
      "Tok-Acc: 0.9412735849056604 Tags: 0.6935123042505593 Tmpl: 0.5774193548387097 Tot: 0.31543624161073824 Tag: 0.2930648769574944\n",
      "Inconsistent for copy mechanism:  0.19015659955257272\n",
      "Template only in Test data:  0.30648769574944074\n",
      "Tok-Acc: 0.9412735849056604 Tags: 0.6935123042505593 Tmpl: 0.5741935483870968 Tot: 0.31543624161073824 Tag: 0.28187919463087246\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,40,5):\n",
    "    run_eval(test, builders, \"End\", \"test\",k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
